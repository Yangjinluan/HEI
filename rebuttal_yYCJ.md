**Our contributions:**

**(a) Problem view:** a new distribution shift perspective(node's neighbor pattern gap between train and test) to promote node representation learning on heterophilic graphs. **Compared with previous works that aim to design a more effective HGNN backbone, we are the first to reconsider the node representation aggregation from data distribution, thus further integrating with existing SOTA HGNN's backbones to achieve better performance(line214-line246)**. Notably, the data distribution means that **we should compare the structure-related distribution between train and test datasets(Figure 7) on the same dataset rather than directly statistics on the full dataset like many other studies.** 

**(b) Technique view:** To address our pointed distribution shift, we make a detailed theoretical analysis to explain why previous graph-based invariant learning methods can't work well(Figure 1, line 271-281,line341, line 381). **Compared with previous works that generate extra augmented graphs by mask strategy to construct different environments(Figure 1, line 312-340), we utilize the node's inherent neighbor pattern information to infer environments without augmentation. Then a natural question arises as bold by lines 147,151, how can we ensure the effectiveness of our proposed graph-based invariant learning methods? Thus, we should provide strict theory evidence for selecting a proper matrix to estimate the node's neighbor pattern from a graph perspective(Section 4.1) and casual invariant learning perspective(Section 4.3)** rather than only from experimental results. A more detailed analysis can be found in the appendix(Figure 5, A.2).



**Reply to Reviewer yYCJ**

**Q1 The significance of the investigated problem** 

Thanks for your questions. As we know, heterophilic graphs are composed of nodes with different levels of homophily. Many previous HGNNS works aim to propose more effective Neighbor aggregation mechanisms to select similar neighbors for each node during neighbor aggregation, thus designing better backbones to achieve good performance on heterophily graph datasets. However, as shown in Table 5, their evaluation is only based on the full test nodes neglecting the difference of test nodes with high and low homohily respectively. 

**From the application view**, take anomaly detection as an example, due to various time factors and the annotation preferences of human experts, the heterophily and homophily can change across training and testing data, and this distribution gap will weaken the generalization of the trained model to detect the anomaly class and the normal classes with different levels of node homophily. To avoid the influence of homophily distribution on model predictions, we should train a model that can perform well on test nodes with high homophily and low homophily simultaneously. Thus, apart from full test accuracy, we should also focus on the performance gap between test groups with different levels of homophily, which corresponds to the results of Table 2 and Table 3. Moreover, as stated in lines 786-789, in real-world scenarios, we should consider the effect of data sampling on the model training. That's why we conduct experiments to further verify it's important to address this distribution shift.

**From the theory view**, the gap between train and test distribution will influence the evaluation of the model's true performance. A model that can perform well on full test nodes may achieve a huge performance gap between the high homophily test and low homophily test, which further verifies the necessity to address this issue. Moreover, we review the data split in Figure 7 and point out this neglected distribution shift.  Based on the causal invariant learning theory, an ideal model should learn the invariant feature that's independent of the environment-related feature so as to adapt to diverse and complex environments. The unstable environments often cause the gap between train and test data distribution, which is independent of the model backbone and related to data distribution, further influencing the model performance. For node classification tasks, due to the potential unlabeled status of neighbor nodes, we can not know whether the train and test split is reasonable considering the node's neighbor pattern distribution. Thus, apart from directly using a fixed backbone to fit the training data, we should also explore the strategy to optimize the backbone to achieve good performance considering the randomness of sampling, which is also a valuable problem.

**Q2 More explanation for similarity matrix**

We first clarify that we don't use three similarity matrices in our framework simultaneously. The theory in 4.1 just verifies that we can use these different matrices to estimate the neighbor pattern z. Then the neighbor pattern can be used for further invariant learning. Exactly,**as stated by the introduction(161-167) and Related work(241-246), the goal we utilize the similarity to evaluate the neighbor pattern is different from previous works. Previous works aim to explore more effective neighbor aggregation mechanisms to select similar neighbors for each node(they belong to backbone design works). But we utilize the neighbor pattern to infer the node's environments for invariant representation learning(it is a framework considering the distribution gap between train and test, that can be integrated with previously designed backbones without difficulty.** In other words, the framework is our contribution rather than the specific somewhat matrix to design an effective backbone. As long as we can find an indicator that meets the conditions stated in the method, our framework can be integrated with previously designed backbones to further improve model performance from a data distribution perspective. **The experiments in Table 4 can also verify that adopting any similarity-based indicator that meets the conditions we clarify in Methods can achieve better performance than the base(previous SOTA backbone itself).**

**Q3 Comparison experiments with other invariant learning methods(e.g.GIL)**

Exactly, we have compare existing all invariant learning methods in our node-level settings. In Related work(lines 256-259), we have discussed the GIL with our work. We focus on node-level OOD generalization on graphs, while GIL focuses on the graph-level OOD problem. Some consistent discussions and statements can be also found in previous works(e.g.EERM[1],FLOOD[2]).

[1] Handling distribution shifts on graphs: An invariance perspective. ICLR2022.

[2] FLOOD: A flexible invariant learning framework for out-of-distribution generalization on graphs. KDD2023.